{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bill-unitech2021/bias_detection_system/blob/main/project_code/Train_using_bert_base_uncased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UhW9VuQOEwly",
        "outputId": "5abb0543-4592-4353-8cb4-b65fce3f9f3e"
      },
      "id": "UhW9VuQOEwly",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPjbkVwQFpo8",
        "outputId": "5df547b4-f038-4fea-8d22-b38300e66c16"
      },
      "id": "OPjbkVwQFpo8",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr  2 13:59:57 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    46W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Au9zMHEZ4bA4",
        "outputId": "074e3295-2899-49a6-eb7c-1e1b9138bd25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Au9zMHEZ4bA4",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60329d77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60329d77",
        "outputId": "75963d87-8a86-42fe-ff1a-5a9b6bee92b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d87923a8",
      "metadata": {
        "id": "d87923a8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0d67948f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d67948f",
        "outputId": "c6725a23-8dd3-44ce-d5a0-694b9a8c421e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff85b3a0",
      "metadata": {
        "id": "ff85b3a0"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ghYHkOFwwp3A"
      },
      "id": "ghYHkOFwwp3A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ee3077dc",
      "metadata": {
        "id": "ee3077dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d697a92f-a05f-48ed-bead-2ba6c38de1d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5045, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/gdrive/MyDrive/bias_detection_syetem/final_dataset.csv/final_dataset.csv\")\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9e2c9681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "9e2c9681",
        "outputId": "1ad347cd-0e2f-4ec2-8139-7aa94bd50aeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0                                        word_labels  \\\n",
              "0            0                          O,O,O,O,O,O,O,O,O,O,O,O,O   \n",
              "1            1  O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,...   \n",
              "2            2  O,O,O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,...   \n",
              "3            3  O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,...   \n",
              "4            4                  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O   \n",
              "5            5  O,O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,O,...   \n",
              "6            6  O,O,O,Behavioural Stereotypes,Behavioural Ster...   \n",
              "7            7  O,O,O,O,O,O,Behavioural Stereotypes,i-Behaviou...   \n",
              "8            8  O,O,O,O,O,O,Behavioural Stereotypes,Behavioura...   \n",
              "9            9  O,O,O,O,O,Behavioural Stereotypes,O,O,Behaviou...   \n",
              "10          10                  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O   \n",
              "11          11  O,O,O,O,Behavioural Stereotypes,O,O,Behavioura...   \n",
              "12          12  O,O,O,O,O,O,O,O,O,O,O,O,Behavioural Stereotype...   \n",
              "13          13  O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,...   \n",
              "14          14  O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,...   \n",
              "15          15        O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O   \n",
              "16          16  O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,O,...   \n",
              "17          17  O,O,O,O,O,O,O,O,O,O,O,O,O,O,Behavioural Stereo...   \n",
              "18          18  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,Behavioural ...   \n",
              "19          19  O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,O,...   \n",
              "\n",
              "                                             sentence  \n",
              "0   ;- Make formal and interpersonal interpretatio...  \n",
              "1   The incumbent should be committed personwith s...  \n",
              "2   The incumbentshould be a quick learner who is ...  \n",
              "3   Knowledge of English is aplus;- Coordination, ...  \n",
              "4   A full time (or part timesee following sentenc...  \n",
              "5   You should have good communication, organisati...  \n",
              "6   They require a confident, well groomed and art...  \n",
              "7   The successful candidate will have good leader...  \n",
              "8   The ideal candidate will have a proactive, dri...  \n",
              "9   You should be a selfmotivated, independent, de...  \n",
              "10  Our client is physically independent, therefor...  \n",
              "11  You will be a logical thinker, highly analytic...  \n",
              "12  You will work to set KPI s; however this is a ...  \n",
              "13  The salary on offer is very competitive, up to...  \n",
              "14  They're looking for a trusted team member who ...  \n",
              "15  Fox Search Ltd is a specialist Recruitment to ...  \n",
              "16  We require a committed, dynamic individual, wh...  \n",
              "17  Having previously worked within a busy Contact...  \n",
              "18  Applications are invited from employees who ar...  \n",
              "19  You will be warm, resilient and enthusiastic a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91b86496-c3f8-432b-8ded-586fbfdbfd6e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>word_labels</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "      <td>;- Make formal and interpersonal interpretatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,...</td>\n",
              "      <td>The incumbent should be committed personwith s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>O,O,O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,...</td>\n",
              "      <td>The incumbentshould be a quick learner who is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,...</td>\n",
              "      <td>Knowledge of English is aplus;- Coordination, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "      <td>A full time (or part timesee following sentenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>O,O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,O,...</td>\n",
              "      <td>You should have good communication, organisati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>O,O,O,Behavioural Stereotypes,Behavioural Ster...</td>\n",
              "      <td>They require a confident, well groomed and art...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>O,O,O,O,O,O,Behavioural Stereotypes,i-Behaviou...</td>\n",
              "      <td>The successful candidate will have good leader...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>O,O,O,O,O,O,Behavioural Stereotypes,Behavioura...</td>\n",
              "      <td>The ideal candidate will have a proactive, dri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>O,O,O,O,O,Behavioural Stereotypes,O,O,Behaviou...</td>\n",
              "      <td>You should be a selfmotivated, independent, de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "      <td>Our client is physically independent, therefor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>O,O,O,O,Behavioural Stereotypes,O,O,Behavioura...</td>\n",
              "      <td>You will be a logical thinker, highly analytic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,Behavioural Stereotype...</td>\n",
              "      <td>You will work to set KPI s; however this is a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>O,O,O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,...</td>\n",
              "      <td>The salary on offer is very competitive, up to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>O,O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,...</td>\n",
              "      <td>They're looking for a trusted team member who ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "      <td>Fox Search Ltd is a specialist Recruitment to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,O,...</td>\n",
              "      <td>We require a committed, dynamic individual, wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,Behavioural Stereo...</td>\n",
              "      <td>Having previously worked within a busy Contact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,Behavioural ...</td>\n",
              "      <td>Applications are invited from employees who ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>O,O,O,Behavioural Stereotypes,O,O,O,O,O,O,O,O,...</td>\n",
              "      <td>You will be warm, resilient and enthusiastic a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91b86496-c3f8-432b-8ded-586fbfdbfd6e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91b86496-c3f8-432b-8ded-586fbfdbfd6e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91b86496-c3f8-432b-8ded-586fbfdbfd6e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d85277af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d85277af",
        "outputId": "9bde48bd-04ca-461f-efb2-5a18096fed4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'O': 0,\n",
              " 'Generic she': 1,\n",
              " 'Generic he': 2,\n",
              " 'Behavioural Stereotypes': 3,\n",
              " 'i-Behavioural Stereotypes': 4,\n",
              " 'Societal Stereotypes': 5,\n",
              " 'i-Societal Stereotypes': 6,\n",
              " 'Explicit Marking of Sex': 7,\n",
              " 'i-Explicit Marking of Sex': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "temp_labels = ['O', 'Generic she', 'Generic he',\n",
        "               'Behavioural Stereotypes', 'i-Behavioural Stereotypes',\n",
        "               'Societal Stereotypes', 'i-Societal Stereotypes',\n",
        "               'Explicit Marking of Sex', 'i-Explicit Marking of Sex']\n",
        "label2id = {k: v for v, k in enumerate(temp_labels)}\n",
        "id2label = {v: k for v, k in enumerate(temp_labels)}\n",
        "label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98dfca0a",
      "metadata": {
        "id": "98dfca0a"
      },
      "source": [
        "### Initialize required parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aedc87f6",
      "metadata": {
        "id": "aedc87f6"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0419d41c",
      "metadata": {
        "id": "0419d41c"
      },
      "source": [
        "### Pytorch dataset class implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "25218f74",
      "metadata": {
        "id": "25218f74"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.sentence[index]  \n",
        "        word_labels = self.data.word_labels[index]  \n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "        \n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "            tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "            labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "            tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "            labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "        \n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        # the following line is deprecated\n",
        "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
        "        \n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d91bdb",
      "metadata": {
        "id": "f6d91bdb"
      },
      "source": [
        "### Divide dataset into test and train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07673a82",
      "metadata": {
        "id": "07673a82"
      },
      "source": [
        "### Helper functions(tokenization, training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ec5cdc15",
      "metadata": {
        "id": "ec5cdc15"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        \n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs[0], outputs[1]\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "        \n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d2838e59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2838e59",
        "outputId": "86779758-3357-4043-8fab-fb64d35144ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (5045, 3)\n",
            "TRAIN Dataset: (4036, 3)\n",
            "TEST Dataset: (1009, 3)\n"
          ]
        }
      ],
      "source": [
        "train_size = 0.8\n",
        "train_dataset = data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0b96e300",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b96e300",
        "outputId": "80175f2c-7be8-4fd8-f5e4-9e644b54a6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]       O\n",
            "professional  O\n",
            ",           O\n",
            "energetic   O\n",
            ",           O\n",
            "positive    O\n",
            "attitude    O\n",
            ",           O\n",
            "self        O\n",
            "-           O\n",
            "motivated   O\n",
            ",           O\n",
            "resource    O\n",
            "##ful       O\n",
            ",           O\n"
          ]
        }
      ],
      "source": [
        "# print the first 30 tokens and corresponding labels\n",
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[2][\"ids\"][:15]), training_set[0][\"targets\"][:15]):\n",
        "    print('{0:10}  {1}'.format(token, id2label[label.item()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac82a39b",
      "metadata": {
        "id": "ac82a39b"
      },
      "source": [
        "### Initialize Pytorch dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "504782e3",
      "metadata": {
        "id": "504782e3"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ab86328e",
      "metadata": {
        "id": "ab86328e"
      },
      "outputs": [],
      "source": [
        "test_dataset.to_csv(\"test_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176ce394",
      "metadata": {
        "id": "176ce394"
      },
      "source": [
        "### Initialize model and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f8b307db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8b307db",
        "outputId": "e0e3f250-1225-493b-ebf3-1f5a4b653106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#model = BertForTokenClassification.from_pretrained(\"model\", from_tf=False) # From already pretrained model\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc9e846",
      "metadata": {
        "id": "0dc9e846"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5830e902",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5830e902",
        "outputId": "4968fa5c-8a0f-42e4-899b-34c29d2fe5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.2516582012176514\n",
            "Training loss per 100 training steps: 0.299881484470155\n",
            "Training loss per 100 training steps: 0.1824265849401257\n",
            "Training loss per 100 training steps: 0.1361747925630786\n",
            "Training loss per 100 training steps: 0.110400559169303\n",
            "Training loss per 100 training steps: 0.0931762745836285\n",
            "Training loss per 100 training steps: 0.08159800967668027\n",
            "Training loss per 100 training steps: 0.07307327958156014\n",
            "Training loss per 100 training steps: 0.06633180220287001\n",
            "Training loss per 100 training steps: 0.06078697097995272\n",
            "Training loss per 100 training steps: 0.056335904867389236\n",
            "Training loss epoch: 0.05617533558522738\n",
            "Training accuracy epoch: 0.9444767410719944\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.02771168388426304\n",
            "Training loss per 100 training steps: 0.014529760643415669\n",
            "Training loss per 100 training steps: 0.013262990656521968\n",
            "Training loss per 100 training steps: 0.012921313613659372\n",
            "Training loss per 100 training steps: 0.012786115605634942\n",
            "Training loss per 100 training steps: 0.012083704193436844\n",
            "Training loss per 100 training steps: 0.011946827401511663\n",
            "Training loss per 100 training steps: 0.011671815538173076\n",
            "Training loss per 100 training steps: 0.01126455085220678\n",
            "Training loss per 100 training steps: 0.011328519819876474\n",
            "Training loss per 100 training steps: 0.011123259768953415\n",
            "Training loss epoch: 0.0110812137445446\n",
            "Training accuracy epoch: 0.979788036868466\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0008336934843100607\n",
            "Training loss per 100 training steps: 0.006391302708393461\n",
            "Training loss per 100 training steps: 0.006589679710627233\n",
            "Training loss per 100 training steps: 0.006994353336370256\n",
            "Training loss per 100 training steps: 0.0071623709982962455\n",
            "Training loss per 100 training steps: 0.007183291258856453\n",
            "Training loss per 100 training steps: 0.006909159629663046\n",
            "Training loss per 100 training steps: 0.006891720812168562\n",
            "Training loss per 100 training steps: 0.006682675570039109\n",
            "Training loss per 100 training steps: 0.006803377342633138\n",
            "Training loss per 100 training steps: 0.006701541768246703\n",
            "Training loss epoch: 0.006686112030363818\n",
            "Training accuracy epoch: 0.9873040200684987\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.0072451005689799786\n",
            "Training loss per 100 training steps: 0.0049447630106190235\n",
            "Training loss per 100 training steps: 0.004647181399626454\n",
            "Training loss per 100 training steps: 0.004323353456214157\n",
            "Training loss per 100 training steps: 0.00421040588737282\n",
            "Training loss per 100 training steps: 0.004421884195410652\n",
            "Training loss per 100 training steps: 0.00441423813054476\n",
            "Training loss per 100 training steps: 0.00439231029908291\n",
            "Training loss per 100 training steps: 0.004386721851838143\n",
            "Training loss per 100 training steps: 0.004273136663096869\n",
            "Training loss per 100 training steps: 0.004341003858140315\n",
            "Training loss epoch: 0.00434007092038787\n",
            "Training accuracy epoch: 0.992031736637458\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.001125141279771924\n",
            "Training loss per 100 training steps: 0.003147201150676494\n",
            "Training loss per 100 training steps: 0.002751083383368518\n",
            "Training loss per 100 training steps: 0.0027218180031463833\n",
            "Training loss per 100 training steps: 0.0028619769328631496\n",
            "Training loss per 100 training steps: 0.0029795806198355326\n",
            "Training loss per 100 training steps: 0.0029706490807343325\n",
            "Training loss per 100 training steps: 0.0031044457592334157\n",
            "Training loss per 100 training steps: 0.003183501271113722\n",
            "Training loss per 100 training steps: 0.0031345900843027445\n",
            "Training loss per 100 training steps: 0.003092078324313137\n",
            "Training loss epoch: 0.0030756496690025384\n",
            "Training accuracy epoch: 0.9942612260237355\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.00041831741691567004\n",
            "Training loss per 100 training steps: 0.0021723024097575557\n",
            "Training loss per 100 training steps: 0.0026773204367442184\n",
            "Training loss per 100 training steps: 0.002319797548810422\n",
            "Training loss per 100 training steps: 0.00226485696787806\n",
            "Training loss per 100 training steps: 0.0022147512422534617\n",
            "Training loss per 100 training steps: 0.002329673489689662\n",
            "Training loss per 100 training steps: 0.0022879893403557113\n",
            "Training loss per 100 training steps: 0.002273427666566139\n",
            "Training loss per 100 training steps: 0.0023339854725073846\n",
            "Training loss per 100 training steps: 0.0023250629573986123\n",
            "Training loss epoch: 0.0023209884526544135\n",
            "Training accuracy epoch: 0.9957420974668432\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.00019173631153535098\n",
            "Training loss per 100 training steps: 0.00120842277620156\n",
            "Training loss per 100 training steps: 0.001613613348638127\n",
            "Training loss per 100 training steps: 0.0015864288470886904\n",
            "Training loss per 100 training steps: 0.001549046187503513\n",
            "Training loss per 100 training steps: 0.0015599915909452157\n",
            "Training loss per 100 training steps: 0.0017088893314659054\n",
            "Training loss per 100 training steps: 0.0017223014809226434\n",
            "Training loss per 100 training steps: 0.0016917271110314653\n",
            "Training loss per 100 training steps: 0.0017385650772262068\n",
            "Training loss per 100 training steps: 0.0017389484078280388\n",
            "Training loss epoch: 0.001749185112280641\n",
            "Training accuracy epoch: 0.9965990179510552\n",
            "Training epoch: 8\n",
            "Training loss per 100 training steps: 0.006697729229927063\n",
            "Training loss per 100 training steps: 0.001263472746935498\n",
            "Training loss per 100 training steps: 0.0014004794047017168\n",
            "Training loss per 100 training steps: 0.0012012210182574473\n",
            "Training loss per 100 training steps: 0.0013115691371129111\n",
            "Training loss per 100 training steps: 0.0014457083090089731\n",
            "Training loss per 100 training steps: 0.001522163750561471\n",
            "Training loss per 100 training steps: 0.0015928363803461574\n",
            "Training loss per 100 training steps: 0.0015896764088666373\n",
            "Training loss per 100 training steps: 0.0015627933269824724\n",
            "Training loss per 100 training steps: 0.0015441973122611566\n",
            "Training loss epoch: 0.0015428094852934832\n",
            "Training accuracy epoch: 0.9968644530865675\n",
            "Training epoch: 9\n",
            "Training loss per 100 training steps: 0.0021491292864084244\n",
            "Training loss per 100 training steps: 0.001131181750643369\n",
            "Training loss per 100 training steps: 0.0010972670068766283\n",
            "Training loss per 100 training steps: 0.0011353435079324249\n",
            "Training loss per 100 training steps: 0.001282789070848181\n",
            "Training loss per 100 training steps: 0.001179661419519833\n",
            "Training loss per 100 training steps: 0.0011771536694032147\n",
            "Training loss per 100 training steps: 0.0012014614831435842\n",
            "Training loss per 100 training steps: 0.0011883267827179271\n",
            "Training loss per 100 training steps: 0.00120832579035713\n",
            "Training loss per 100 training steps: 0.0012122669349201596\n",
            "Training loss epoch: 0.001219771045538305\n",
            "Training accuracy epoch: 0.9976621548689947\n",
            "Training epoch: 10\n",
            "Training loss per 100 training steps: 4.8250003601424396e-05\n",
            "Training loss per 100 training steps: 0.0010008725178405518\n",
            "Training loss per 100 training steps: 0.0008763638021689678\n",
            "Training loss per 100 training steps: 0.0008808245227531572\n",
            "Training loss per 100 training steps: 0.0009563127513912488\n",
            "Training loss per 100 training steps: 0.000992328361433523\n",
            "Training loss per 100 training steps: 0.001100933289887754\n",
            "Training loss per 100 training steps: 0.0010943653909941224\n",
            "Training loss per 100 training steps: 0.0010379165411794839\n",
            "Training loss per 100 training steps: 0.0010908546882618109\n",
            "Training loss per 100 training steps: 0.0010944724991885425\n",
            "Training loss epoch: 0.0010893533094274556\n",
            "Training accuracy epoch: 0.9978087293113946\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62795b4a",
      "metadata": {
        "id": "62795b4a"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b0ce95c0",
      "metadata": {
        "id": "b0ce95c0"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "            \n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs[0], outputs[1]\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "    \n",
        "    #print(eval_labels)\n",
        "    #print(eval_preds)\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "    #print(labels)\n",
        "    #print(predictions)\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5f022c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f022c0c",
        "outputId": "60965551-9cb2-453b-d862-b0ecc98ec214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.012863784097135067\n",
            "Validation loss per 100 evaluation steps: 0.005800888254694061\n",
            "Validation loss per 100 evaluation steps: 0.005934603725190347\n",
            "Validation loss per 100 evaluation steps: 0.006637101803995314\n",
            "Validation loss per 100 evaluation steps: 0.007711593484512144\n",
            "Validation loss per 100 evaluation steps: 0.008181750064345933\n",
            "Validation Loss: 0.0081175861988242\n",
            "Validation Accuracy: 0.9896531373905291\n"
          ]
        }
      ],
      "source": [
        "labels, predictions = valid(model, testing_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624867de",
      "metadata": {
        "id": "624867de"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "caa375b2",
      "metadata": {
        "id": "caa375b2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "folder_name = \"/content/gdrive/MyDrive/bias_detection_syetem/bias_detection_model_bert-base-uncased\" # Pick name and path for the folder you want to save the model in\n",
        "os.makedirs(folder_name)\n",
        "model.save_pretrained(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "89b811e0",
      "metadata": {
        "id": "89b811e0",
        "outputId": "a97ddc39-f392-4b93-a0f6-641f9884fb0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'they will support agencies to develop their campaigns to their needs with proactive support, advice and expertise.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "test_dataset.sentence[12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2910bbf5",
      "metadata": {
        "id": "2910bbf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43168ec8-1a6a-439d-e373-de28208b3b71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1009, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "test_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49d97ee5",
      "metadata": {
        "id": "49d97ee5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}