{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8511c332",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b222c48",
   "metadata": {},
   "source": [
    "## Step 1: initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7f557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ea67c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#initialize the lematizer and stemmer, which will be used later.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#to be used in the cleaning function\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "#print(stopwords.words('chinese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430196d",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Data\n",
    "The dataset, EMSCAD, is downloaded from Kaggle, the link is here:\n",
    "https://www.kaggle.com/datasets/amruthjithrajvr/recruitment-scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a7cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is (17880, 18)\n"
     ]
    }
   ],
   "source": [
    "#load the EMSCAD dataset\n",
    "df = pd.read_csv('./DataSet.csv')\n",
    "print(\"The shape of the dataframe is\",df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291776c",
   "metadata": {},
   "source": [
    "### Get the dataframe for only the Job description column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9a1457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is (17880, 1)\n",
      "Display one Job description sample:\n",
      "\n",
      "<p>Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.</p>\r\n",
      "<ul>\r\n",
      "<li>Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systems</li>\r\n",
      "<li>Researching blogs and websites for the Provisions by Food52 Affiliate Program</li>\r\n",
      "<li>Assisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiries</li>\r\n",
      "<li>Supporting with PR &amp; Events when needed</li>\r\n",
      "<li>Helping with office administrative work, such as filing, mailing, and preparing for meetings</li>\r\n",
      "<li>Working with developers to document bugs and suggest improvements to the site</li>\r\n",
      "<li>Supporting the marketing and executive staff</li>\r\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "df_jd = pd.DataFrame(df['description'])\n",
    "print(\"The shape of the dataframe is\", df_jd.shape) \n",
    "print(\"Display one Job description sample:\\n\") \n",
    "print(df_jd['description'][0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c922e7b",
   "metadata": {},
   "source": [
    "### To display the Job description sample in a more readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09204a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.</p>\r\n",
       "<ul>\r\n",
       "<li>Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systems</li>\r\n",
       "<li>Researching blogs and websites for the Provisions by Food52 Affiliate Program</li>\r\n",
       "<li>Assisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiries</li>\r\n",
       "<li>Supporting with PR &amp; Events when needed</li>\r\n",
       "<li>Helping with office administrative work, such as filing, mailing, and preparing for meetings</li>\r\n",
       "<li>Working with developers to document bugs and suggest improvements to the site</li>\r\n",
       "<li>Supporting the marketing and executive staff</li>\r\n",
       "</ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(df_jd['description'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ea9af",
   "metadata": {},
   "source": [
    "### Prepare the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614351e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the HTML tags\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # remove the HTML tags\n",
    "    text = striphtml(text)\n",
    "    \n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.replace(':', ' ')\n",
    "    text = text.replace('\\'', ' ')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove extra spaces from text\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Remove stopwords function\n",
    "    # Tokenize : get a list of tokens\n",
    "    stop_words = set(stopwords.words(\"english\")) # nltk.download('stopwords') - this is done at the begining\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    text = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    \n",
    "    # Stem words\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b80cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['organis', 'focu', 'vibrant', 'awesomedo', 'passion', 'custom', 'servic', 'slick', 'type', 'skill', 'mayb', 'account', 'manag', 'think', 'administr', 'cooler', 'polar', 'bear', 'jetski', 'need', 'hear', 'cloud', 'video', 'product', 'servic', 'opper', 'glodal', 'level', 'yeah', 'pretti', 'cool', 'seriou', 'deliv', 'world', 'class', 'product', 'excel', 'custom', 'serviceour', 'rapidli', 'expand', 'busi', 'look', 'talent', 'project', 'manag', 'manag', 'success', 'deliveri', 'video', 'project', 'manag', 'client', 'commun', 'drive', 'product', 'process', 'work', 'coolest', 'brand', 'planet', 'learn', 'global', 'team', 'repres', 'nz', 'huge', 'way', 'enter', 'next', 'growth', 'stage', 'busi', 'grow', 'quickli', 'intern', 'therefor', 'posit', 'burst', 'opportun', 'right', 'person', 'enter', 'busi', 'right', 'time', '90', 'second', 'world', 'cloud', 'video', 'product', 'servic', 'http', '90urlfbe6559afac620a3cd2c22281f7b8d0eef56a73e3d9a311e2f1ca13d081dd630', '90', 'second', 'world', 'cloud', 'video', 'product', 'servic', 'enabl', 'brand', 'agenc', 'get', 'high', 'qualiti', 'onlin', 'video', 'content', 'shoot', 'produc', 'anywher', 'world', 'fast', 'afford', 'manag', 'seamlessli', 'cloud', 'purchas', 'publish', '90', 'second', 'remov', 'hassl', 'cost', 'risk', 'speed', 'issu', 'work', 'regular', 'video', 'product', 'compani', 'manag', 'everi', 'aspect', 'video', 'project', 'beauti', 'onlin', 'experi', 'grow', 'network', '2000', 'rat', 'video', 'profession', '50', 'countri', 'dedic', 'product', 'success', 'team', '5', 'countri', 'guarante', 'video', 'project', 'success', '100', 'easi', 'commiss', 'quick', 'googl', 'adword', 'campaign90', 'second', 'produc', 'almost', '4000', 'video', '30', 'countri', '500', 'global', 'brand', 'includ', 'world', 'largest', 'includ', 'paypal', 'l', 'oreal', 'soni', 'barclay', 'offic', 'auckland', 'london', 'sydney', 'tokyo', 'amp', 'singaporeour', 'auckland', 'offic', 'base', 'right', 'heart', 'wynyard', 'quarter', 'innov', 'precinct', 'gridakl']\n",
      "['client', 'locat', 'houston', 'activ', 'seek', 'experi', 'commiss', 'machineri', 'assist', 'possess', 'strong', 'supervisori', 'skill', 'attent', 'detail', 'strong', 'dedic', 'safeti', 'must', 'ideal', 'candid', 'execut', 'activ', 'compli', 'qualiti', 'requir', 'health', 'environment', 'safeti', 'regul']\n",
      "['compani', 'esri', '–', 'environment', 'system', 'research', 'institut', 'passion', 'improv', 'qualiti', 'life', 'geographi', 'heart', 'everyth', 'esri', '’', 'geograph', 'inform', 'system', 'gi', 'technolog', 'inspir', 'enabl', 'govern', 'univers', 'busi', 'worldwid', 'save', 'money', 'live', 'environ', 'deeper', 'understand', 'chang', 'world', 'around', 'care', 'manag', 'growth', 'zero', 'debt', 'give', 'esri', 'stabil', 'uncommon', 'today', 'volatil', 'busi', 'world', 'privat', 'hold', 'offer', 'except', 'benefit', 'competit', 'salari', '401k', 'profitshar', 'program', 'opportun', 'person', 'profession', 'growth', 'much', 'opportun', 'account', 'execut', 'member', 'sale', 'divis', 'work', 'collabor', 'account', 'team', 'order', 'sell', 'promot', 'adopt', 'esri', '’', 'arcgi', 'platform', 'within', 'organ', 'part', 'account', 'team', 'respons', 'facilit', 'develop', 'execut', 'set', 'strategi', 'defin', 'portfolio', 'account', 'execut', 'strategi', 'util', 'experi', 'enterpris', 'sale', 'help', 'custom', 'leverag', 'geospati', 'inform', 'technolog', 'achiev', 'busi', 'goal', 'specifically…', 'prospect', 'develop', 'opportun', 'partner', 'key', 'stakehold', 'envis', 'develop', 'implement', 'locat', 'strategi', 'organ', 'clearli', 'articul', 'strength', 'valu', 'proposit', 'arcgi', 'platform', 'develop', 'maintain', 'healthi', 'pipelin', 'opportun', 'busi', 'growth', 'demonstr', 'thought', 'understand', 'insight', 'industri', 'knowledg', 'gi', 'appli', 'initi', 'trend', 'trigger', 'understand', 'key', 'busi', 'driver', 'within', 'organ', 'identifi', 'key', 'busi', 'stakehold', 'understand', 'custom', '’', 'budget', 'acquisit', 'process', 'success', 'execut', 'account', 'manag', 'process', 'includ', 'account', 'priorit', 'account', 'resourc', 'account', 'plan', 'success', 'execut', 'sale', 'process', 'opportun', 'leverag', 'lead', 'account', 'team', 'consist', 'sale', 'crossdivision', 'resourc', 'defin', 'execut', 'account', 'strategi', 'effect', 'util', 'leverag', 'crm', 'manag', 'opportun', 'drive', 'buy', 'process', 'pursu', 'profession', 'person', 'develop', 'ensur', 'competit', 'knowledg', 'real', 'estat', 'industri', 'leverag', 'social', 'media', 'success', 'prospect', 'build', 'profession', 'network', 'particip', 'trade', 'show', 'workshop', 'seminar', 'requir', 'support', 'visual', 'stori', 'tell', 'effect', 'whiteboard', 'session', 'resourc', 'take', 'initi', 'resolv', 'issu']\n",
      "['job', 'titl', 'item', 'review', 'manag', 'locat', 'fort', 'worth', 'tx', 'depart', 'item', 'review', 'report', 'vp', 'oper', 'gener', 'descript', 'respons', 'overal', 'aspect', 'item', 'review', 'oper', 'personnel', 'hire', 'qualiti', 'control', 'process', 'workflow', 'monitor', 'track', 'account', 'staff', 'regard', 'product', 'standard', 'depart', 'expect', 'duti', 'respons', 'overse', 'compani', '’', 'item', 'review', 'depart', 'oper', 'respons', 'encourag', 'reinforc', 'compani', 'cultur', 'develop', 'process', 'better', 'depart', 'implement', 'new', 'proceduresprotocol', 'work', 'custom', 'servic', 'elev', 'issu', 'provid', 'call', 'implement', 'audit', 'polici', 'conjunct', 'polici', 'payment', 'integr', 'depart', 'monitor', 'qualityand', 'qualiti', 'control', 'result', 'depart', 'respons', 'ensur', 'overal', 'metric', 'complianc', 'manag', 'client', 'expect', 'respons', 'human', 'resourc', 'matter', 'directli', 'relat', 'depart', 'supervis', 'ie', 'interview', 'hire', 'train', 'annual', 'evalu', 'electron', 'time', 'card', 'address', 'personnel', 'issu', 'may', 'createreview', 'daili', 'weekli', 'monthli', 'report', 'invoic', 'log', 'expens', 'addit', 'dutiesrespons', 'assign', 'compli', 'safeti', 'rulesregul', 'conjunct', 'injuri', 'ill', 'prevent', 'program', '“', 'iipp', '”', 'well', 'maintain', 'hipaa', 'complianc', 'occasion', 'interact', 'custom']\n"
     ]
    }
   ],
   "source": [
    "# for now, I only used some of the examples (df_jd['description'][1:100]), due to the computing capability.\n",
    "\n",
    "df_jd_sentence = pd.DataFrame()\n",
    "\n",
    "for index, sentence in df_jd['description'][1:5].iteritems(): \n",
    "    #df_jd_sentence.add(clean(sentence))\n",
    "    print(clean(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5f889",
   "metadata": {},
   "source": [
    "##  For the purpose of demonstration, I did a cleaning process step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d061b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.</p>\r\n",
      "<ul>\r\n",
      "<li>Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systems</li>\r\n",
      "<li>Researching blogs and websites for the Provisions by Food52 Affiliate Program</li>\r\n",
      "<li>Assisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiries</li>\r\n",
      "<li>Supporting with PR &amp; Events when needed</li>\r\n",
      "<li>Helping with office administrative work, such as filing, mailing, and preparing for meetings</li>\r\n",
      "<li>Working with developers to document bugs and suggest improvements to the site</li>\r\n",
      "<li>Supporting the marketing and executive staff</li>\r\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "# define new variables, \"text_jd\" and \"text_jd_without_stopwords\", which will only be used here for demonstration\n",
    "# and only use one job description as example.\n",
    "text_jd = df_jd['description'][0]\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8e44ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.\r\n",
      "\r\n",
      "Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systems\r\n",
      "Researching blogs and websites for the Provisions by Food52 Affiliate Program\r\n",
      "Assisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiries\r\n",
      "Supporting with PR &amp; Events when needed\r\n",
      "Helping with office administrative work, such as filing, mailing, and preparing for meetings\r\n",
      "Working with developers to document bugs and suggest improvements to the site\r\n",
      "Supporting the marketing and executive staff\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove the HTML tags\n",
    "text_jd = striphtml(text_jd)\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89faedb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food52, a fast-growing, james beard award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its new york city headquarters.\r\n",
      "\r\n",
      "reproducing and/or repackaging existing food52 content for a number of partner sites, such as huffington post, yahoo, buzzfeed, and more in their various content management systems\r\n",
      "researching blogs and websites for the provisions by food52 affiliate program\r\n",
      "assisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiries\r\n",
      "supporting with pr &amp; events when needed\r\n",
      "helping with office administrative work, such as filing, mailing, and preparing for meetings\r\n",
      "working with developers to document bugs and suggest improvements to the site\r\n",
      "supporting the marketing and executive staff\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase text\n",
    "text_jd = text_jd.lower()\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f01101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food52 a fastgrowing james beard awardwinning online food community and crowdsourced and curated recipe hub is currently interviewing full and parttime unpaid interns to work in a small team of editors executives and developers in its new york city headquarters\r\n",
      "\r\n",
      "reproducing andor repackaging existing food52 content for a number of partner sites such as huffington post yahoo buzzfeed and more in their various content management systems\r\n",
      "researching blogs and websites for the provisions by food52 affiliate program\r\n",
      "assisting in daytoday affiliate program support such as screening affiliates and assisting in any affiliate inquiries\r\n",
      "supporting with pr amp events when needed\r\n",
      "helping with office administrative work such as filing mailing and preparing for meetings\r\n",
      "working with developers to document bugs and suggest improvements to the site\r\n",
      "supporting the marketing and executive staff\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove ':', '\\', and punctuation\n",
    "text_jd = text_jd.replace(':', ' ')\n",
    "text_jd = text_jd.replace('\\'', ' ')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text_jd = text_jd.translate(translator)\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb47f8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food52 a fastgrowing james beard awardwinning online food community and crowdsourced and curated recipe hub is currently interviewing full and parttime unpaid interns to work in a small team of editors executives and developers in its new york city headquarters reproducing andor repackaging existing food52 content for a number of partner sites such as huffington post yahoo buzzfeed and more in their various content management systems researching blogs and websites for the provisions by food52 affiliate program assisting in daytoday affiliate program support such as screening affiliates and assisting in any affiliate inquiries supporting with pr amp events when needed helping with office administrative work such as filing mailing and preparing for meetings working with developers to document bugs and suggest improvements to the site supporting the marketing and executive staff\n"
     ]
    }
   ],
   "source": [
    "# Remove extra spaces from text\n",
    "text_jd = \" \".join(text_jd.split())\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa4abdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"don't\", \"hadn't\", 'these', \"haven't\", 'we', 'for', 'more', 'against', 'both', 'weren', 'here', 'in', \"aren't\", \"you're\", 'over', 'off', 'too', 'and', 'd', 'other', 'its', 'not', \"she's\", 'yourself', 'just', 'out', 'yourselves', 'to', \"weren't\", 're', 'below', 'isn', 'under', 'ain', 'than', \"wouldn't\", 'o', 'couldn', 'such', 'was', 'when', 've', 'how', 'needn', 'again', 'were', 'am', 'be', 'ours', 'while', 'will', 'above', 'shan', 'there', 'hers', 'my', 'do', 'why', 'from', \"you'll\", 'of', 'few', \"hasn't\", 'those', 'but', 'which', 'between', 'about', 'mightn', 'with', 'ma', \"mustn't\", \"isn't\", 'theirs', 'or', 'this', 'then', 'are', 'whom', 'by', 'wouldn', 'can', 'his', 'nor', 'me', 'her', 'before', \"couldn't\", 'doesn', 'haven', 'is', 'had', \"you've\", 'what', 'itself', 'same', 'once', \"should've\", \"wasn't\", 'hasn', 'doing', 'they', 'now', \"you'd\", 'ourselves', 'a', 'it', 'have', 'themselves', \"needn't\", 'through', 'i', 'up', 'so', \"didn't\", 'shouldn', 'hadn', 'their', 'been', 'didn', 'them', 'your', 'where', 'down', 'wasn', \"won't\", 'won', 'at', 'until', 'because', 'she', 'should', 'any', 'during', \"it's\", 'who', 'no', 'that', 'yours', 'as', 'on', 'having', 'further', 'y', \"that'll\", 'has', 'aren', 'myself', 'himself', 'if', \"mightn't\", 'll', 'being', 'don', 'into', \"doesn't\", 'him', 'he', 'the', 'only', 'herself', 'most', 'some', 'you', 'mustn', 'all', 't', 'does', 'very', 'after', 'an', 's', \"shouldn't\", 'did', 'own', 'our', 'm', \"shan't\", 'each'}\n"
     ]
    }
   ],
   "source": [
    "# set stopwords \n",
    "stop_words = set(stopwords.words(\"english\")) # nltk.download('stopwords') - this is done at the begining\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d670007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food52', 'fastgrowing', 'james', 'beard', 'awardwinning', 'online', 'food', 'community', 'crowdsourced', 'curated', 'recipe', 'hub', 'currently', 'interviewing', 'full', 'parttime', 'unpaid', 'interns', 'work', 'small', 'team', 'editors', 'executives', 'developers', 'new', 'york', 'city', 'headquarters', 'reproducing', 'andor', 'repackaging', 'existing', 'food52', 'content', 'number', 'partner', 'sites', 'huffington', 'post', 'yahoo', 'buzzfeed', 'various', 'content', 'management', 'systems', 'researching', 'blogs', 'websites', 'provisions', 'food52', 'affiliate', 'program', 'assisting', 'daytoday', 'affiliate', 'program', 'support', 'screening', 'affiliates', 'assisting', 'affiliate', 'inquiries', 'supporting', 'pr', 'amp', 'events', 'needed', 'helping', 'office', 'administrative', 'work', 'filing', 'mailing', 'preparing', 'meetings', 'working', 'developers', 'document', 'bugs', 'suggest', 'improvements', 'site', 'supporting', 'marketing', 'executive', 'staff']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize : get a list of tokens\n",
    "# Remove stop words\n",
    "word_tokens = word_tokenize(text_jd)\n",
    "text_jd_without_stopwords = [word for word in word_tokens if word not in stop_words]\n",
    "print(text_jd_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2140d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food52', 'fastgrowing', 'jam', 'beard', 'awardwinning', 'online', 'food', 'community', 'crowdsourced', 'curated', 'recipe', 'hub', 'currently', 'interview', 'full', 'parttime', 'unpaid', 'intern', 'work', 'small', 'team', 'editors', 'executives', 'developers', 'new', 'york', 'city', 'headquarter', 'reproduce', 'andor', 'repackaging', 'exist', 'food52', 'content', 'number', 'partner', 'sit', 'huffington', 'post', 'yahoo', 'buzzfeed', 'various', 'content', 'management', 'systems', 'research', 'blog', 'websites', 'provision', 'food52', 'affiliate', 'program', 'assist', 'daytoday', 'affiliate', 'program', 'support', 'screen', 'affiliate', 'assist', 'affiliate', 'inquiries', 'support', 'pr', 'amp', 'events', 'need', 'help', 'office', 'administrative', 'work', 'file', 'mail', 'prepare', 'meet', 'work', 'developers', 'document', 'bug', 'suggest', 'improvements', 'site', 'support', 'market', 'executive', 'staff']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize words\n",
    "text_jd_without_stopwords = [lemmatizer.lemmatize(word, pos ='v') for word in text_jd_without_stopwords]\n",
    "print(text_jd_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "880c0460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food52', 'fastgrow', 'jam', 'beard', 'awardwin', 'onlin', 'food', 'commun', 'crowdsourc', 'curat', 'recip', 'hub', 'current', 'interview', 'full', 'parttim', 'unpaid', 'intern', 'work', 'small', 'team', 'editor', 'execut', 'develop', 'new', 'york', 'citi', 'headquart', 'reproduc', 'andor', 'repackag', 'exist', 'food52', 'content', 'number', 'partner', 'sit', 'huffington', 'post', 'yahoo', 'buzzfe', 'variou', 'content', 'manag', 'system', 'research', 'blog', 'websit', 'provis', 'food52', 'affili', 'program', 'assist', 'daytoday', 'affili', 'program', 'support', 'screen', 'affili', 'assist', 'affili', 'inquiri', 'support', 'pr', 'amp', 'event', 'need', 'help', 'offic', 'administr', 'work', 'file', 'mail', 'prepar', 'meet', 'work', 'develop', 'document', 'bug', 'suggest', 'improv', 'site', 'support', 'market', 'execut', 'staff']\n"
     ]
    }
   ],
   "source": [
    "# Stem words \n",
    "text_jd_without_stopwords = [stemmer.stem(word) for word in text_jd_without_stopwords]\n",
    "print(text_jd_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fe3e",
   "metadata": {},
   "source": [
    "# Step 2: preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc14caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 2)\n",
      "  Biased Words or Phrases Masculine/Feminine Bias\n",
      "0                  active          Masculine Bias\n",
      "1             adventurous          Masculine Bias\n",
      "2                 aggress          Masculine Bias\n",
      "3                 ambitio          Masculine Bias\n",
      "4                   analy          Masculine Bias\n",
      "Masculine Bias    95\n",
      "Feminine Bias     64\n",
      "Name: Masculine/Feminine Bias, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get the list of biased words or phrases\n",
    "df_biased_words = pd.read_excel(\"./bias_words.xlsx\")\n",
    "print(df_biased_words.shape)\n",
    "print(df_biased_words.head())\n",
    "print(df_biased_words['Masculine/Feminine Bias'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3941048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 2)\n",
      "Biased Words or Phrases    False\n",
      "Masculine/Feminine Bias    False\n",
      "dtype: bool\n",
      "  Biased Words or Phrases Masculine/Feminine Bias\n",
      "0                  active          Masculine Bias\n",
      "1             adventurous          Masculine Bias\n",
      "2                 aggress          Masculine Bias\n",
      "3                 ambitio          Masculine Bias\n",
      "4                   analy          Masculine Bias\n",
      "Masculine Bias    81\n",
      "Feminine Bias     56\n",
      "Name: Masculine/Feminine Bias, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# exclude generic he/she and dupulicated words in df_biased_words\n",
    "words_to_exclude = ['she', 'her', 'hers', 'herself', 'he', 'himself', 'him', 'his']\n",
    "df_biased_words = df_biased_words[~df_biased_words['Biased Words or Phrases'].isin(words_to_exclude)]\n",
    "df_biased_words = df_biased_words.drop_duplicates()\n",
    "\n",
    "print(df_biased_words.shape)\n",
    "print(df_biased_words.isna().any())# check if there's any empty cells\n",
    "print(df_biased_words.head())\n",
    "print(df_biased_words['Masculine/Feminine Bias'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3248f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of male_words and female words\n",
    "male_words = (df_biased_words.loc[df_biased_words['Masculine/Feminine Bias'] == 'Masculine Bias'])['Biased Words or Phrases'].values\n",
    "female_words = (df_biased_words.loc[df_biased_words['Masculine/Feminine Bias'] == 'Feminine Bias'])['Biased Words or Phrases'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb6ff917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['active' 'adventurous' 'aggress' 'ambitio' 'analy' 'assert' 'athlet'\n",
      " 'autonom' 'boast' 'challeng' 'compet' 'confident' 'courag' 'decide'\n",
      " 'decisive' 'decision' 'determin' 'dominant' 'domina' 'force' 'greedy'\n",
      " 'headstrong' 'hierarch' 'hostil' 'implusive' 'independen' 'individual'\n",
      " 'intellect' 'lead' 'logic' 'masculine' 'objective' 'opinion' 'outspoken'\n",
      " 'persist' 'principle' 'reckless' 'stubborn' 'superior' 'self-confiden'\n",
      " 'self-sufficien' 'self-relian' 'manmade' 'chairman' 'son' 'fireman'\n",
      " 'freshman' 'man' 'mankind' 'manpower' 'boyfriend' 'husband' 'policeman'\n",
      " 'walter' 'brother' 'spokesman' 'upperclassman' 'gentleman' 'alumnus'\n",
      " 'alumni' 'man up' 'Mr.' 'man-made' 'the common man' 'mailman' 'steward'\n",
      " 'actor' 'congressman' 'acts as a leader' 'aggressive' 'ambitious'\n",
      " 'analytical' 'assertive' 'athletic' 'competitive' 'defends own beliefs'\n",
      " 'forceful' 'has leadership abilities' 'independent' 'individualistic'\n",
      " 'makes decisions easily']\n"
     ]
    }
   ],
   "source": [
    "print(male_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7388b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affectionate' 'child' 'cheer' 'commit' 'communal' 'compassion' 'connect'\n",
      " 'considerate' 'cooperat' 'depend' 'emotiona' 'empath' 'feminine'\n",
      " 'flatterable' 'gentle' 'honest' 'interpersonal' 'interdependen'\n",
      " 'interpersona' 'kind' 'kinship' 'loyal' 'modesty' 'nag' 'nurtur'\n",
      " 'pleasant' 'polite' 'quiet' 'respon' 'sensitiv' 'submissive' 'support'\n",
      " 'sympath' 'tender' 'together' 'trust' 'understand' 'warm' 'whin' 'yield'\n",
      " 'daughter' 'wife' 'girlfriend' 'waitress' 'sister' 'ladies' 'alumna'\n",
      " 'alumnae' 'hysterical' 'shrill' 'nagging' 'Mrs.' 'Miss.' 'Ms.'\n",
      " 'stewardess' 'actress']\n"
     ]
    }
   ],
   "source": [
    "print(female_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bec6b3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndf_new = pd.DataFrame()\\n\\nfor index, sentence in df_sentences.iterrows():\\n    temp_sentence = sentence[\\'sentences\\']\\n    tokenized_sentence = clean(temp_sentence)\\n    category = \\'neutral\\'\\n    word_in_sentence = \\'None\\'\\n    word = \\'None\\'\\n    \\n    # check for male words\\n    for male_word in male_words:\\n        if re.search(r\"\\x08{}\\x08\".format(male_word), temp_sentence.lower().strip()):\\n            category = \\'masculine\\'\\n            word_in_sentence = male_word\\n            word = male_word\\n        else:\\n            for token in tokenized_sentence:\\n                if len(male_word) > 3:\\n\\n                    if simple_clean(male_word) == token[:len(male_word)]:\\n                        category = \\'masculine\\'\\n                        word_in_sentence = token\\n                        word = male_word\\n                    elif simple_clean(male_word) == token[-len(male_word):]:\\n                        category = \\'masculine\\'\\n                        word_in_sentence = token\\n                        word = male_word\\n            \\n    if category == \\'masculine\\':\\n        dict = {\\'sentence\\': temp_sentence,\\n                \\'word_in_Sentence\\': word_in_sentence,\\n                \\'biased_term\\': word,\\n                \\'category\\': category\\n               }\\n        df_new = df_new.append(dict, ignore_index = True)\\n        \\n    # check for female words\\n    for female_word in female_words:\\n        if re.search(r\"\\x08{}\\x08\".format(female_word), temp_sentence.lower().strip()):\\n            category = \\'feminine\\'\\n            word_in_sentence = female_word\\n            word = female_word\\n        else:\\n            for token in tokenized_sentence:\\n                if len(female_word) > 3:\\n                    if simple_clean(female_word) == token[:len(female_word)]:\\n                        category = \\'feminine\\'\\n                        word_in_sentence = token\\n                        word = female_word\\n                    elif simple_clean(female_word) == token[-len(female_word):]:\\n                        category = \\'feminine\\'\\n                        word_in_sentence = token\\n                        word = female_word\\n                    \\n    if category == \\'feminine\\':\\n        dict = {\\'sentence\\': temp_sentence,\\n                \\'word_in_Sentence\\': word_in_sentence,\\n                \\'biased_term\\': word,\\n                \\'category\\': category\\n               }\\n        df_new = df_new.append(dict, ignore_index = True)\\n    \\n    if index%10000 == 0:\\n        print(index)\\n        \\n        \\n        \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "for index, sentence in df_sentences.iterrows():\n",
    "    temp_sentence = sentence['sentences']\n",
    "    tokenized_sentence = clean(temp_sentence)\n",
    "    category = 'neutral'\n",
    "    word_in_sentence = 'None'\n",
    "    word = 'None'\n",
    "    \n",
    "    # check for male words\n",
    "    for male_word in male_words:\n",
    "        if re.search(r\"\\b{}\\b\".format(male_word), temp_sentence.lower().strip()):\n",
    "            category = 'masculine'\n",
    "            word_in_sentence = male_word\n",
    "            word = male_word\n",
    "        else:\n",
    "            for token in tokenized_sentence:\n",
    "                if len(male_word) > 3:\n",
    "\n",
    "                    if simple_clean(male_word) == token[:len(male_word)]:\n",
    "                        category = 'masculine'\n",
    "                        word_in_sentence = token\n",
    "                        word = male_word\n",
    "                    elif simple_clean(male_word) == token[-len(male_word):]:\n",
    "                        category = 'masculine'\n",
    "                        word_in_sentence = token\n",
    "                        word = male_word\n",
    "            \n",
    "    if category == 'masculine':\n",
    "        dict = {'sentence': temp_sentence,\n",
    "                'word_in_Sentence': word_in_sentence,\n",
    "                'biased_term': word,\n",
    "                'category': category\n",
    "               }\n",
    "        df_new = df_new.append(dict, ignore_index = True)\n",
    "        \n",
    "    # check for female words\n",
    "    for female_word in female_words:\n",
    "        if re.search(r\"\\b{}\\b\".format(female_word), temp_sentence.lower().strip()):\n",
    "            category = 'feminine'\n",
    "            word_in_sentence = female_word\n",
    "            word = female_word\n",
    "        else:\n",
    "            for token in tokenized_sentence:\n",
    "                if len(female_word) > 3:\n",
    "                    if simple_clean(female_word) == token[:len(female_word)]:\n",
    "                        category = 'feminine'\n",
    "                        word_in_sentence = token\n",
    "                        word = female_word\n",
    "                    elif simple_clean(female_word) == token[-len(female_word):]:\n",
    "                        category = 'feminine'\n",
    "                        word_in_sentence = token\n",
    "                        word = female_word\n",
    "                    \n",
    "    if category == 'feminine':\n",
    "        dict = {'sentence': temp_sentence,\n",
    "                'word_in_Sentence': word_in_sentence,\n",
    "                'biased_term': word,\n",
    "                'category': category\n",
    "               }\n",
    "        df_new = df_new.append(dict, ignore_index = True)\n",
    "    \n",
    "    if index%10000 == 0:\n",
    "        print(index)\n",
    "        \n",
    "        \n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16df0470",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_988\\64920552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This is the first sentence. This is the second sentence! This is the third sentence? And this is the fourth sentence.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \"\"\"\n\u001b[1;32m-> 1276\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \"\"\"\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \"\"\"\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \"\"\"\n\u001b[0;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1395\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[0mbefore_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m             \u001b[1;31m# Ignore matches that have already been captured by matches to the right of this match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmatches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbefore_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# try the sent_tokenize() function\n",
    "paragraph = \"This is the first sentence. This is the second sentence! This is the third sentence? And this is the fourth sentence.\"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "words = word_tokenize(clean(paragraph))\n",
    "print(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c27b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc17592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56def600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca4117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4e9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765133f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698aeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff27227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab9379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f769f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3861882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be9f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2872b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad3242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f32b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13454d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca0a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
