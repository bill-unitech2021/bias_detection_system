{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8511c332",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b222c48",
   "metadata": {},
   "source": [
    "## Step 1: initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the lematizer and stemmer, which will be used later.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#to be used in the cleaning function\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "#print(stopwords.words('chinese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430196d",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Data\n",
    "The dataset, EMSCAD, is downloaded from Kaggle, the link is here:\n",
    "https://www.kaggle.com/datasets/amruthjithrajvr/recruitment-scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the EMSCAD dataset\n",
    "df = pd.read_csv('./DataSet.csv')\n",
    "print(\"The shape of the dataframe is\",df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291776c",
   "metadata": {},
   "source": [
    "### Get the dataframe for only the Job description column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jd = pd.DataFrame(df['description'])\n",
    "print(\"The shape of the dataframe is\", df_jd.shape) \n",
    "print(\"Display one Job description sample:\\n\") \n",
    "print(df_jd['description'][0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c922e7b",
   "metadata": {},
   "source": [
    "### To display the Job description sample in a more readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09204a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(df_jd['description'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ea9af",
   "metadata": {},
   "source": [
    "### Prepare the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614351e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the HTML tags\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # remove the HTML tags\n",
    "    text = striphtml(text)\n",
    "    \n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.replace(':', ' ')\n",
    "    text = text.replace('\\'', ' ')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove extra spaces from text\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Remove stopwords function\n",
    "    # Tokenize : get a list of tokens\n",
    "    stop_words = set(stopwords.words(\"english\")) # nltk.download('stopwords') - this is done at the begining\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    text = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    \n",
    "    # Stem words\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc682dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_clean(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    #text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b80cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, I only used some of the examples (df_jd['description'][1:100]), due to the computing capability.\n",
    "\n",
    "for index, sentence in df_jd['description'][1:2].iteritems(): \n",
    "    #df_jd_sentence.add(clean(sentence))\n",
    "    print(clean(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5f889",
   "metadata": {},
   "source": [
    "##  For the purpose of demonstration, I did a cleaning process step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d061b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new variables, \"text_jd\" and \"text_jd_without_stopwords\", which will only be used here for demonstration\n",
    "# and only use one job description as example.\n",
    "text_jd = df_jd['description'][0]\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the HTML tags\n",
    "text_jd = striphtml(text_jd)\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89faedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase text\n",
    "text_jd = text_jd.lower()\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f01101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ':', '\\', and punctuation\n",
    "text_jd = text_jd.replace(':', ' ')\n",
    "text_jd = text_jd.replace('\\'', ' ')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text_jd = text_jd.translate(translator)\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces from text\n",
    "text_jd = \" \".join(text_jd.split())\n",
    "print(text_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stopwords \n",
    "stop_words = set(stopwords.words(\"english\")) # nltk.download('stopwords') - this is done at the begining\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize : get a list of tokens\n",
    "# Remove stop words\n",
    "word_tokens = word_tokenize(text_jd)\n",
    "text_jd_without_stopwords = [word for word in word_tokens if word not in stop_words]\n",
    "print(text_jd_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize words\n",
    "text_jd_without_stopwords = [lemmatizer.lemmatize(word, pos ='v') for word in text_jd_without_stopwords]\n",
    "print(text_jd_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words \n",
    "text_jd_without_stopwords = [stemmer.stem(word) for word in text_jd_without_stopwords]\n",
    "print(text_jd_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fe3e",
   "metadata": {},
   "source": [
    "# Step 2: preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc14caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of biased words or phrases\n",
    "df_biased_words = pd.read_excel(\"./bias_words.xlsx\")\n",
    "print(df_biased_words.shape)\n",
    "print(df_biased_words.head())\n",
    "print(df_biased_words['Masculine/Feminine Bias'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude generic he/she and dupulicated words in df_biased_words\n",
    "words_to_exclude = ['she', 'her', 'hers', 'herself', 'he', 'himself', 'him', 'his']\n",
    "df_biased_words = df_biased_words[~df_biased_words['Biased Words or Phrases'].isin(words_to_exclude)]\n",
    "df_biased_words = df_biased_words.drop_duplicates()\n",
    "\n",
    "print(df_biased_words.shape)\n",
    "print(df_biased_words.isna().any())# check if there's any empty cells\n",
    "print(df_biased_words.head())\n",
    "print(df_biased_words['Masculine/Feminine Bias'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3248f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of male_words and female words\n",
    "male_words = (df_biased_words.loc[df_biased_words['Masculine/Feminine Bias'] == 'Masculine Bias'])['Biased Words or Phrases'].values\n",
    "female_words = (df_biased_words.loc[df_biased_words['Masculine/Feminine Bias'] == 'Feminine Bias'])['Biased Words or Phrases'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ff917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(male_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7388b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(female_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start to test for checking male and female worsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56def600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'sentences': ['Must be an extrovert with an innate quality of easily connecting with people.', \n",
    "                      'You are self-motivated and decisive, but willing to make changes with minimal grumbling when the client demands it.', \n",
    "                      'We are looking for a young and driven candidate who can bring innovation into the organization.']}\n",
    "\n",
    "df_test_sentence = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fac07",
   "metadata": {},
   "source": [
    "## The following block is to find out in each sentence if there is any male or female words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_result = pd.DataFrame()\n",
    "\n",
    "for index, sentence in df_test_sentence.iterrows():\n",
    "    temp_sentence = sentence['sentences']\n",
    "    tokenized_sentence = clean(temp_sentence)\n",
    "    \n",
    "    # there will be columns named 'category', 'word_in_sentences', and 'biased_term' in 'df_check_result' \n",
    "    category = 'neutral' \n",
    "    word_in_sentence = 'None' # \n",
    "    word = 'None' # \n",
    "    \n",
    "    # check for male words, and them put the outcome to \n",
    "    for male_word in male_words:\n",
    "        if re.search(r\"\\b{}\\b\".format(male_word), temp_sentence.lower().strip()): # search for 'male_word' in 'temp_sentence' using RE\n",
    "            # set output if 'male_word' is found\n",
    "            category = 'masculine'\n",
    "            word_in_sentence = male_word\n",
    "            word = male_word\n",
    "            #when there is no male word in the temp_sentence \n",
    "        else:\n",
    "            for token in tokenized_sentence:\n",
    "                if len(male_word) > 3:\n",
    "                    if simple_clean(male_word) == token[:len(male_word)]: # check if the male_word is found at the beginning of the token\n",
    "                        category = 'masculine'\n",
    "                        word_in_sentence = token\n",
    "                        word = male_word\n",
    "                    elif simple_clean(male_word) == token[-len(male_word):]: # check if the male_word is found at the end of the token\n",
    "                        category = 'masculine'\n",
    "                        word_in_sentence = token\n",
    "                        word = male_word\n",
    "            \n",
    "    if category == 'masculine': # put the outcome in a dict, then append them to 'df_check_result'\n",
    "        dict = {'sentence': temp_sentence,\n",
    "                'word_in_Sentence': word_in_sentence,\n",
    "                'biased_term': word,\n",
    "                'category': category\n",
    "               }\n",
    "        #df_check_result = df_check_result.append(dict, ignore_index = True)\n",
    "        df_check_result = pd.concat([df_check_result, pd.DataFrame([dict])], ignore_index=True)\n",
    "\n",
    "        \n",
    "        \n",
    "    # the completely same process for checking for female words\n",
    "    for female_word in female_words:\n",
    "        if re.search(r\"\\b{}\\b\".format(female_word), temp_sentence.lower().strip()):\n",
    "            category = 'feminine'\n",
    "            word_in_sentence = female_word\n",
    "            word = female_word\n",
    "        else:\n",
    "            for token in tokenized_sentence:\n",
    "                if len(female_word) > 3:\n",
    "                    if simple_clean(female_word) == token[:len(female_word)]:\n",
    "                        category = 'feminine'\n",
    "                        word_in_sentence = token\n",
    "                        word = female_word\n",
    "                    elif simple_clean(female_word) == token[-len(female_word):]:\n",
    "                        category = 'feminine'\n",
    "                        word_in_sentence = token\n",
    "                        word = female_word\n",
    "                    \n",
    "    if category == 'feminine':\n",
    "        dict = {'sentence': temp_sentence,\n",
    "                'word_in_Sentence': word_in_sentence,\n",
    "                'biased_term': word,\n",
    "                'category': category\n",
    "               }\n",
    "        #df_check_result = df_check_result.append(dict, ignore_index = True)\n",
    "        df_check_result = pd.concat([df_check_result, pd.DataFrame([dict])], ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # This is to roughly monitor how many lines/sentences have been proceeded when running this block of code\n",
    "    #if index%10000 == 0:\n",
    "        #print(f'{index} sentences have been processed.' )\n",
    "        \n",
    "    print(f'{index + 1} sentences have been processed.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765133f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the reult frome checking male/female words are now put into 'df_check_result'\n",
    "df_check_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046b447",
   "metadata": {},
   "source": [
    "## This block is to find out male or female words in sentences as well, only to store outcomes in different form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using df_jd_sentences to store sentences extracted from EMSCAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame()\n",
    "\n",
    "for index, sentence in df_test_sentence.iterrows():\n",
    "    temp_sentence = sentence['sentences']\n",
    "    tokenized_sentence = clean(temp_sentence)\n",
    "    words_in_sentence = []\n",
    "    words = []\n",
    "    \n",
    "    if len(temp_sentence) < 180 and temp_sentence[0].isupper():\n",
    "        # check for male words\n",
    "        for male_word in male_words:\n",
    "            if re.search(r\"\\b{}\\b\".format(male_word), temp_sentence.lower().strip()):\n",
    "                words_in_sentence.append([male_word, 'M'])\n",
    "                words.append([male_word, 'M'])\n",
    "            else:\n",
    "                for token in tokenized_sentence:\n",
    "                    if len(male_word) > 3:\n",
    "                        if simple_clean(male_word) == token[:len(male_word)]:\n",
    "                            words_in_sentence.append([token, 'M'])\n",
    "                            words.append([male_word, 'M'])\n",
    "                        elif simple_clean(male_word) == token[-len(male_word):]:\n",
    "                            if token[:len(male_word)] != token[-len(male_word):]:\n",
    "                                words_in_sentence.append([token, 'M'])\n",
    "                                words.append([male_word, 'M'])\n",
    "\n",
    "        # check for female words\n",
    "        for female_word in female_words:\n",
    "            if re.search(r\"\\b{}\\b\".format(female_word), temp_sentence.lower().strip()):\n",
    "                words_in_sentence.append([female_word, 'F'])\n",
    "                words.append([male_word, 'F'])\n",
    "            else:\n",
    "                for token in tokenized_sentence:\n",
    "                    if len(female_word) > 3:\n",
    "                        if simple_clean(female_word) == token[:len(female_word)]:\n",
    "                            words_in_sentence.append([token, 'F'])\n",
    "                            words.append([female_word, 'F'])\n",
    "                        elif simple_clean(female_word) == token[-len(female_word):]:\n",
    "                            if token[:len(female_word)] != token[-len(female_word):]:\n",
    "                                words_in_sentence.append([token, 'F'])\n",
    "                                words.append([female_word, 'F'])\n",
    "\n",
    "        if len(words) > 0:\n",
    "            dict = {'sentence': temp_sentence,\n",
    "                    'word_in_Sentence': words_in_sentence,\n",
    "                    'biased_term': words}\n",
    "            df_new = df_new.append(dict, ignore_index = True)\n",
    "\n",
    "        if index%10000 == 0:\n",
    "            print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a77a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    temp = \" \".join(sentence.split())\n",
    "    temp = temp.strip()\n",
    "\n",
    "    char_to_remove = 0\n",
    "    for x in temp.split()[0]:\n",
    "        if not x.isalpha():\n",
    "            char_to_remove += 1\n",
    "    temp = temp[char_to_remove: len(temp) - char_to_remove]\n",
    "    return sentence\n",
    "\n",
    "'''\n",
    "In Pandas, the map() method is used to apply a function to every element of a Series object\n",
    "or a column of a DataFrame object. The map() method takes a function as an argument and applies\n",
    "it to each element of the Series or column, returning a new Series or column with the results of\n",
    "the function applied to each element.\n",
    "'''\n",
    "\n",
    "#cleaning the values in the 'sentence' column of 'df_new' using the 'clean_sentence' function\n",
    "df_new['sentence'] = df_new.sentence.map(lambda x:clean_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b26e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_in_Sentence</th>\n",
       "      <th>biased_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Must be an extrovert with an innate quality of...</td>\n",
       "      <td>[[connect, F]]</td>\n",
       "      <td>[[connect, F]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are self-motivated and decisive, but willi...</td>\n",
       "      <td>[[decisive, M]]</td>\n",
       "      <td>[[decisive, M]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence word_in_Sentence  \\\n",
       "0  Must be an extrovert with an innate quality of...   [[connect, F]]   \n",
       "1  You are self-motivated and decisive, but willi...  [[decisive, M]]   \n",
       "\n",
       "       biased_term  \n",
       "0   [[connect, F]]  \n",
       "1  [[decisive, M]]  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa07770",
   "metadata": {},
   "source": [
    "### explore filtered sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286d3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "545f1bfd",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35a012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13293f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c505a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
